# LiS Chatbot

Ein vollst√§ndiger Chatbot mit Text- und Sprach-Ein-/Ausgabe, verbunden mit Supabase und OpenAI.

## Features

- üí¨ Text-Chat mit OpenAI GPT-4o
- üé§ Sprach-Eingabe (Speech-to-Text) mit Deepgram
- üîä Sprach-Ausgabe (Text-to-Speech) mit ElevenLabs
- üóÑÔ∏è Supabase Datenbank-Integration
- üì± Mobile-optimiertes Design

## Technologie-Stack

- **Next.js 14** - React Framework
- **TypeScript** - Type Safety
- **Tailwind CSS** - Styling
- **OpenAI API** - LLM f√ºr Chat
- **Supabase** - Datenbank
- **Deepgram** - Speech-to-Text
- **ElevenLabs** - Text-to-Speech

## Lokale Entwicklung

1. **Dependencies installieren:**
   ```bash
   npm install
   ```

2. **Umgebungsvariablen einrichten:**
   Erstelle eine `.env.local` Datei mit folgenden Variablen:
   ```env
   OPENAI_API_KEY=dein_openai_key
   SUPABASE_URL=deine_supabase_url
   SUPABASE_SERVICE_ROLE_KEY=dein_service_role_key
   DEEPGRAM_API_KEY=dein_deepgram_key
   ELEVENLABS_API_KEY=dein_elevenlabs_key
   ELEVENLABS_VOICE_ID=deine_voice_id (optional, Standard: Rachel)
   INTERNAL_API_KEY=geheimer_schluessel_fuer_api_zugriff
   NEXT_PUBLIC_INTERNAL_API_KEY=gleicher_schluessel_fuer_den_client
   ```

3. **Development Server starten:**
   ```bash
   npm run dev
   ```

4. **√ñffne** [http://localhost:3000](http://localhost:3000)

## Deployment auf Vercel

### Option 1: Via Vercel CLI (Empfohlen)

1. **Vercel CLI installieren** (falls noch nicht installiert):
   ```bash
   npm install -g vercel
   ```

2. **Login:**
   ```bash
   vercel login
   ```

3. **Deploy:**
   ```bash
   vercel
   ```

4. **Umgebungsvariablen setzen:**
   Gehe zu [Vercel Dashboard](https://vercel.com/dashboard) ‚Üí Dein Projekt ‚Üí Settings ‚Üí Environment Variables
   
   F√ºge alle Variablen aus `.env.local` hinzu:
   - `OPENAI_API_KEY`
   - `SUPABASE_URL`
   - `SUPABASE_SERVICE_ROLE_KEY`
   - `DEEPGRAM_API_KEY`
   - `ELEVENLABS_API_KEY`
   - `ELEVENLABS_VOICE_ID` (optional)

5. **Production Deploy:**
   ```bash
   vercel --prod
   ```

### Option 2: Via GitHub Integration

1. **Code zu GitHub pushen:**
   ```bash
   git add .
   git commit -m "Ready for deployment"
   git push origin main
   ```

2. **Vercel Dashboard:**
   - Gehe zu [vercel.com](https://vercel.com)
   - Klicke auf "New Project"
   - Verbinde dein GitHub Repository
   - Vercel erkennt automatisch Next.js

3. **Umgebungsvariablen setzen:**
   - Im Vercel Dashboard ‚Üí Settings ‚Üí Environment Variables
   - F√ºge alle Variablen hinzu (siehe oben)

4. **Deploy:**
   - Vercel deployed automatisch bei jedem Push zu `main`

### Sicher mit API Keys umgehen

- **Keine Secrets commiten:** Die Datei `.env.local` ist bereits im `.gitignore` aufgef√ºhrt. Trage dort lokal deine echten Schl√ºssel ein, aber pushe die Datei niemals. F√ºr geteilte Beispiele verwende nur Platzhalterwerte.
- **Vercel-Dashboards nutzen:** Hinterlege produktive Keys ausschlie√ülich im Vercel-UI unter *Settings ‚Üí Environment Variables*. So bleiben sie aus dem Code-Repository und Build-Logs heraus.
- **Rotation einplanen:** Erzeuge bei Bedarf neue OpenAI-Schl√ºssel und ersetze den alten Wert in `.env.local` bzw. im Vercel-Dashboard. L√∂sche anschlie√üend den alten Schl√ºssel im OpenAI-Account, falls er kompromittiert sein k√∂nnte.
- **Zugriff einschr√§nken:** Gib Teammitgliedern nur die notwendigen Rollen im OpenAI- und Vercel-Workspace. Teilst du Builds mit anderen, nutze separate Projekt-Keys statt deinen pers√∂nlichen Hauptschl√ºssel.

## Wichtige Hinweise

- **HTTPS erforderlich:** Die Mikrofon-API funktioniert nur √ºber HTTPS (oder localhost). Daher ist Hosting auf Vercel empfohlen.
- **API Keys:** Stelle sicher, dass alle API Keys in Vercel gesetzt sind. F√ºr alle API-Routen (`/api/chat`, `/api/stt`, `/api/tts`) muss der Header `x-api-key` mit `INTERNAL_API_KEY` gesendet werden. Damit der Client diesen Header senden kann, muss derselbe Wert zus√§tzlich als `NEXT_PUBLIC_INTERNAL_API_KEY` bereitgestellt werden.
- **Ratenbegrenzung:** Die API-Routen begrenzen Anfragen pro Minute (z.B. `/api/chat` 30 Anfragen/Minute, `/api/stt` 20 Anfragen/Minute, `/api/tts` 30 Anfragen/Minute) basierend auf der Quell-IP. Bei √úberschreitung wird ein 429-Fehler mit `Retry-After` Header zur√ºckgegeben.
- **Health Check:** `GET /api/health` liefert den aktuellen Status der ben√∂tigten Umgebungsvariablen (ohne Werte offenzulegen), inkl. des Client-Keys `NEXT_PUBLIC_INTERNAL_API_KEY`, und gibt bei fehlenden Variablen HTTP 503 zur√ºck. Optionale Werte wie `ELEVENLABS_VOICE_ID` werden ebenfalls als vorhanden/nicht vorhanden ausgewiesen. Nutze den Endpunkt f√ºr Monitoring oder Deployment-Validierung.
- **Supabase:** Verwende den Service Role Key f√ºr Admin-Zugriff auf die Datenbank.

## Schnelltests (lokal)

Nutze diese Befehle, um die Absicherungen schnell zu pr√ºfen:

1. **Linting:**
   ```bash
   npm run lint
   ```
2. **Health-Check:**
   ```bash
   curl -i http://localhost:3000/api/health
   ```
   - Erwartet: `200 OK` wenn alle ben√∂tigten Variablen gesetzt sind, sonst `503 Service Unavailable` mit `ready: false` im JSON.
3. **Autorisierung der API-Routen:**
   ```bash
   curl -i -H "x-api-key: $INTERNAL_API_KEY" \
     -H "Content-Type: application/json" \
     -d '{"messages":[{"role":"user","content":"Hallo"}]}' \
     http://localhost:3000/api/chat
   ```
   - Erwartet: Ohne g√ºltigen Schl√ºssel `401 Unauthorized`; mit korrektem Schl√ºssel `200 OK` oder ein g√ºltiger Fehlercode der Upstream-Provider. Analog kannst du `/api/stt` (POST mit Audio) und `/api/tts` (POST mit `text`) testen.
4. **Ratenbegrenzung sichtbar machen:** Wiederhole den Aufruf einer Route schnell n-mal; bei √úberschreitung erscheint `429 Too Many Requests` mit `Retry-After` und `X-RateLimit-*` Headern.

## Kann ich es jetzt nutzen?

Ja, wenn folgende Voraussetzungen erf√ºllt sind:

- **Umgebungsvariablen gesetzt:** Alle Pflicht-Keys (OpenAI, Supabase, Deepgram, ElevenLabs, `INTERNAL_API_KEY` und `NEXT_PUBLIC_INTERNAL_API_KEY`) m√ºssen vorhanden sein; pr√ºfe mit `GET /api/health`.
- **Authentifizierte Aufrufe:** Jeder Client-Request an `/api/chat`, `/api/stt` und `/api/tts` muss den Header `x-api-key: $INTERNAL_API_KEY` senden. Der Client nutzt daf√ºr automatisch `NEXT_PUBLIC_INTERNAL_API_KEY`.
- **Grenzen beachten:** Chat begrenzt Nachrichtenanzahl/-l√§nge; STT erlaubt nur unterst√ºtzte Audio-MIME-Typen und Gr√∂√üen; TTS limitiert Textl√§nge. Bei √úberschreitung kommen 400er- oder 429-Antworten mit Hinweisen.

Bekannte Einschr√§nkungen, die du einplanen solltest:

- **Supabase-RLS fehlt noch:** Der Service-Role-Key wird weiterhin serverseitig genutzt; setze das System nicht dem Internet aus, bevor RLS/Least-Privilege umgesetzt ist.
- **Keine Inhaltsmoderation:** Eingaben werden nicht auf toxische/PII-Inhalte gepr√ºft; betreibe nur in kontrollierten Umgebungen.
- **Kein Streaming/Observability:** Antworten werden nicht gestreamt und es fehlen Telemetriedaten f√ºr Kosten/Fehler. Rechne mit l√§ngeren Antwortzeiten und begrenzter Einsicht.

## Wie es weitergeht (Empfohlene n√§chsten Schritte)

1. **Supabase absichern**
   - Aktiviere Row-Level Security f√ºr alle Tabellen und greife aus der Anwendung nur noch √ºber RLS-gesch√ºtzte Views/Policies zu.
   - Ersetze den Service-Role-Key im Runtime-Pfad durch einen User-bezogenen Token, damit Anfragen die richtigen Policies erben.

2. **Inhalts-Moderation hinzuf√ºgen**
   - F√ºhre vor jedem OpenAI-Aufruf eine Moderationspr√ºfung durch (z.B. OpenAI Moderation API) und blocke oder entsch√§rfe toxische/PII-haltige Eingaben.
   - Erg√§nze UI-Hinweise, die bei Blockierungen eine verst√§ndliche Begr√ºndung liefern.

3. **Streaming & Kontext-Optimierung umsetzen**
   - Schalte Token-Streaming f√ºr Chat-Antworten ein und schreibe den Client auf inkrementelles Rendering um.
   - K√ºrze den System-Prompt/Verlauf (z.B. nur letzte N-Nachrichten) oder fasse √§ltere Eintr√§ge zusammen, um Tokenkosten zu senken.

4. **Beobachtbarkeit und Kostenkontrolle**
   - Sammle strukturierte Server-Logs (Latenzen, Fehlerraten, Token-Usage) und setze Warnungen/Alerts auf Ausrei√üer.
   - Erg√§nze einfache Concurrency-Limits pro Route, damit parallele Anfragen nicht unkontrolliert eskalieren.

5. **Regressionen fr√ºh erkennen**
   - F√ºhre `npm run lint` und `npm run build` lokal aus, bevor du neue Deployments anst√∂√üt.
   - Erg√§nze zeitnah automatisierte Tests (Unit + E2E) f√ºr Auth-Zwang, Ratenbegrenzung, Moderation und Streaming-Rendering.

## Browser-Unterst√ºtzung

- ‚úÖ Chrome (Desktop & Mobile)
- ‚úÖ Firefox (Desktop & Mobile)
- ‚úÖ Safari (iOS 14.3+, macOS Safari 11+)
- ‚ö†Ô∏è Safari auf macOS ben√∂tigt HTTPS f√ºr Mikrofon-Zugriff

## Lizenz

Private Projekt

