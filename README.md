# LiS Chatbot

Ein vollst√§ndiger Chatbot mit Text- und Sprach-Ein-/Ausgabe, verbunden mit Supabase und OpenAI.

## Features

- üí¨ Text-Chat mit OpenAI GPT-4o
- üé§ Sprach-Eingabe (Speech-to-Text) mit Deepgram
- üîä Sprach-Ausgabe (Text-to-Speech) mit ElevenLabs
- üóÑÔ∏è Supabase Datenbank-Integration
- üì± Mobile-optimiertes Design

## Technologie-Stack

- **Next.js 14** - React Framework
- **TypeScript** - Type Safety
- **Tailwind CSS** - Styling
- **OpenAI API** - LLM f√ºr Chat
- **Supabase** - Datenbank
- **Deepgram** - Speech-to-Text
- **ElevenLabs** - Text-to-Speech

## Lokale Entwicklung

1. **Dependencies installieren:**
   ```bash
   npm install
   ```

2. **Umgebungsvariablen einrichten:**
   Erstelle eine `.env.local` Datei mit folgenden Variablen:
   ```env
   OPENAI_API_KEY=dein_openai_key
   SUPABASE_URL=deine_supabase_url
   SUPABASE_SERVICE_ROLE_KEY=dein_service_role_key
   DEEPGRAM_API_KEY=dein_deepgram_key
   ELEVENLABS_API_KEY=dein_elevenlabs_key
   ELEVENLABS_VOICE_ID=deine_voice_id (optional, Standard: Rachel)
   INTERNAL_API_KEY=geheimer_schluessel_fuer_api_zugriff
   NEXT_PUBLIC_INTERNAL_API_KEY=gleicher_schluessel_fuer_den_client
   ```

3. **Development Server starten:**
   ```bash
   npm run dev
   ```

4. **√ñffne** [http://localhost:3000](http://localhost:3000)

## Deployment auf Vercel

### Option 1: Via Vercel CLI (Empfohlen)

1. **Vercel CLI installieren** (falls noch nicht installiert):
   ```bash
   npm install -g vercel
   ```

2. **Login:**
   ```bash
   vercel login
   ```

3. **Deploy:**
   ```bash
   vercel
   ```

4. **Umgebungsvariablen setzen:**
   Gehe zu [Vercel Dashboard](https://vercel.com/dashboard) ‚Üí Dein Projekt ‚Üí Settings ‚Üí Environment Variables
   
   F√ºge alle Variablen aus `.env.local` hinzu:
   - `OPENAI_API_KEY`
   - `SUPABASE_URL`
   - `SUPABASE_SERVICE_ROLE_KEY`
   - `DEEPGRAM_API_KEY`
   - `ELEVENLABS_API_KEY`
   - `ELEVENLABS_VOICE_ID` (optional)

5. **Production Deploy:**
   ```bash
   vercel --prod
   ```

### Option 2: Via GitHub Integration

1. **Code zu GitHub pushen:**
   ```bash
   git add .
   git commit -m "Ready for deployment"
   git push origin main
   ```

2. **Vercel Dashboard:**
   - Gehe zu [vercel.com](https://vercel.com)
   - Klicke auf "New Project"
   - Verbinde dein GitHub Repository
   - Vercel erkennt automatisch Next.js

3. **Umgebungsvariablen setzen:**
   - Im Vercel Dashboard ‚Üí Settings ‚Üí Environment Variables
   - F√ºge alle Variablen hinzu (siehe oben)
   - Setze **INTERNAL_API_KEY** und **NEXT_PUBLIC_INTERNAL_API_KEY** auf denselben Wert im Vercel-UI und l√∂se danach ein neues Deploy/Rebuild aus, damit der Build die Werte sieht. Pr√ºfe mit `GET /api/health`, ob die Keys aktiv sind.

4. **Deploy:**
   - Vercel deployed automatisch bei jedem Push zu `main`

### Sicher mit API Keys umgehen

- **Keine Secrets commiten:** Die Datei `.env.local` ist bereits im `.gitignore` aufgef√ºhrt. Trage dort lokal deine echten Schl√ºssel ein, aber pushe die Datei niemals. F√ºr geteilte Beispiele verwende nur Platzhalterwerte.
- **Vercel-Dashboards nutzen:** Hinterlege produktive Keys ausschlie√ülich im Vercel-UI unter *Settings ‚Üí Environment Variables*. So bleiben sie aus dem Code-Repository und Build-Logs heraus.
- **Rotation einplanen:** Erzeuge bei Bedarf neue OpenAI-Schl√ºssel und ersetze den alten Wert in `.env.local` bzw. im Vercel-Dashboard. L√∂sche anschlie√üend den alten Schl√ºssel im OpenAI-Account, falls er kompromittiert sein k√∂nnte.
- **Zugriff einschr√§nken:** Gib Teammitgliedern nur die notwendigen Rollen im OpenAI- und Vercel-Workspace. Teilst du Builds mit anderen, nutze separate Projekt-Keys statt deinen pers√∂nlichen Hauptschl√ºssel.

## Wichtige Hinweise

- **HTTPS erforderlich:** Die Mikrofon-API funktioniert nur √ºber HTTPS (oder localhost). Daher ist Hosting auf Vercel empfohlen.
- **API Keys:** Stelle sicher, dass alle API Keys in Vercel gesetzt sind. F√ºr alle API-Routen (`/api/chat`, `/api/stt`, `/api/tts`) muss der Header `x-api-key` mit `INTERNAL_API_KEY` gesendet werden. Damit der Client diesen Header senden kann, muss derselbe Wert zus√§tzlich als `NEXT_PUBLIC_INTERNAL_API_KEY` bereitgestellt werden.
- **Ratenbegrenzung:** Die API-Routen begrenzen Anfragen pro Minute (z.B. `/api/chat` 30 Anfragen/Minute, `/api/stt` 20 Anfragen/Minute, `/api/tts` 30 Anfragen/Minute) basierend auf der Quell-IP. Bei √úberschreitung wird ein 429-Fehler mit `Retry-After` Header zur√ºckgegeben.
- **Gleichzeitige Anfragen:** Um √úberlast zu vermeiden, lehnen `/api/chat` und `/api/tts` mehr als 6 gleichzeitige Requests und `/api/stt` mehr als 4 gleichzeitige Requests pro Instanz mit `429` ab. Die Header `X-Concurrency-Limit` und `X-Concurrency-Active` zeigen die Grenzen und aktiven Slots an.
- **Health Check:** `GET /api/health` liefert den aktuellen Status der ben√∂tigten Umgebungsvariablen (ohne Werte offenzulegen), inkl. des Client-Keys `NEXT_PUBLIC_INTERNAL_API_KEY`, und gibt bei fehlenden Variablen HTTP 503 zur√ºck. Optionale Werte wie `ELEVENLABS_VOICE_ID` werden ebenfalls als vorhanden/nicht vorhanden ausgewiesen. Nutze den Endpunkt f√ºr Monitoring oder Deployment-Validierung.
- **Supabase:** Verwende den Service Role Key f√ºr Admin-Zugriff auf die Datenbank.

## Schnelltests (lokal)

Nutze diese Befehle, um die Absicherungen schnell zu pr√ºfen:

1. **Linting:**
   ```bash
   npm run lint
   ```
2. **Health-Check:**
   ```bash
   curl -i http://localhost:3000/api/health
   ```
   - Erwartet: `200 OK` wenn alle ben√∂tigten Variablen gesetzt sind, sonst `503 Service Unavailable` mit `ready: false` im JSON.
3. **Autorisierung der API-Routen:**
   ```bash
   curl -i -H "x-api-key: $INTERNAL_API_KEY" \
     -H "Content-Type: application/json" \
     -d '{"messages":[{"role":"user","content":"Hallo"}]}' \
     http://localhost:3000/api/chat
   ```
   - Erwartet: Ohne g√ºltigen Schl√ºssel `401 Unauthorized`; mit korrektem Schl√ºssel `200 OK` oder ein g√ºltiger Fehlercode der Upstream-Provider. Analog kannst du `/api/stt` (POST mit Audio) und `/api/tts` (POST mit `text`) testen.
4. **Ratenbegrenzung sichtbar machen:** Wiederhole den Aufruf einer Route schnell n-mal; bei √úberschreitung erscheint `429 Too Many Requests` mit `Retry-After` und `X-RateLimit-*` Headern.
5. **Moderations-Block testen:** Sende einen klar sch√§dlichen/beleidigenden Beispieltext an `/api/chat` oder `/api/tts` (oder nutze STT mit entsprechender Sprachnachricht); der Server sollte mit `400` und einer Moderations-Fehlermeldung antworten.

## Asset-Richtlinie (keine Bin√§rdateien)

- Halte das Repo frei von Bin√§rdateien (z. B. `.png`, `.jpg`, `.ico`). Vektor-Grafiken wie `.svg` sind textbasiert, diff-freundlich und lassen sich einfach im Review anpassen.
- Wenn du eine bestehende Bin√§rdatei ersetzen musst, konvertiere sie bitte in ein SVG oder ein anderes textbasiertes Format, damit Branch-Updates ohne ‚ÄûBinary files are not supported‚Äú-Fehler durchlaufen.

## Kann ich es jetzt nutzen?

Ja, wenn folgende Voraussetzungen erf√ºllt sind:

- **Umgebungsvariablen gesetzt:** Alle Pflicht-Keys (OpenAI, Supabase, Deepgram, ElevenLabs, `INTERNAL_API_KEY` und `NEXT_PUBLIC_INTERNAL_API_KEY`) m√ºssen vorhanden sein; pr√ºfe mit `GET /api/health`.
- **Authentifizierte Aufrufe:** Jeder Client-Request an `/api/chat`, `/api/stt` und `/api/tts` muss den Header `x-api-key: $INTERNAL_API_KEY` senden. Der Client nutzt daf√ºr automatisch `NEXT_PUBLIC_INTERNAL_API_KEY`.
- **Grenzen beachten:** Chat begrenzt Nachrichtenanzahl/-l√§nge; STT erlaubt nur unterst√ºtzte Audio-MIME-Typen und Gr√∂√üen; TTS limitiert Textl√§nge. Bei √úberschreitung kommen 400er- oder 429-Antworten mit Hinweisen.

Bekannte Einschr√§nkungen, die du einplanen solltest:

- **Supabase-RLS fehlt noch:** Der Service-Role-Key wird weiterhin serverseitig genutzt; setze das System nicht dem Internet aus, bevor RLS/Least-Privilege umgesetzt ist.
- **Kein Streaming/Observability:** Antworten werden nicht gestreamt und es fehlen Telemetriedaten f√ºr Kosten/Fehler. Rechne mit l√§ngeren Antwortzeiten und begrenzter Einsicht.
- **Moderation nur grundlegend:** Chat-, STT- und TTS-Eingaben werden per OpenAI-Moderation gepr√ºft und blockiert, aber es gibt noch keine zus√§tzlichen Filter/Redactions f√ºr PII oder Custom-Richtlinien.

## Status der Verbesserungen (Kurz√ºberblick)

- [x] API-Key-Pflicht und Ratenbegrenzung f√ºr Chat, STT und TTS
- [x] Payload-Validierung (Nachrichtenanzahl/-l√§nge, Audio-MIME/Gr√∂√üe, TTS-Textl√§nge)
- [x] Basis-Moderation f√ºr Chat, STT und TTS
- [ ] Supabase mit RLS und Least-Privilege-Token statt Service-Role-Key
- [ ] Streaming-Antworten und Kontext-Kompaktion zur Latenz- und Kostenreduktion
- [ ] Beobachtbarkeit (strukturierte Logs/Alerts) und Concurrency-Governance
- [ ] PII-Filter/Redactions und policy-spezifische Moderationsregeln

## Was jetzt konkret zu tun ist

1. **Deployment pr√ºfen:** Nach jedem Key- oder Code-Update `GET /api/health` aufrufen und sicherstellen, dass `ready: true` zur√ºckkommt.
2. **Supabase absichern:** RLS aktivieren, Policies f√ºr die ben√∂tigten Views/Tabellen definieren und den Service-Role-Key aus dem Request-Pfad entfernen.
3. **Moderation erweitern:** PII-Redactions/Allowlists erg√§nzen und geblockte Inhalte mit klaren UI-Hinweisen quittieren.
4. **Streaming & Telemetrie einf√ºhren:** Chat-Antworten streamen, √§ltere Verlaufsanteile komprimieren und strukturierte Logs/Metriken f√ºr Latenz, Token und Fehler ausgeben.
5. **Regressionstests erg√§nzen:** Automatisierte Tests f√ºr Auth-Zwang, Raten-/Concurrency-Limits, Moderationspfade und Streaming-Rendering aufsetzen.

## Wie es weitergeht (Empfohlene n√§chsten Schritte)

1. **Supabase absichern**
   - Aktiviere Row-Level Security f√ºr alle Tabellen und greife aus der Anwendung nur noch √ºber RLS-gesch√ºtzte Views/Policies zu.
   - Ersetze den Service-Role-Key im Runtime-Pfad durch einen User-bezogenen Token, damit Anfragen die richtigen Policies erben.

2. **Moderation ausbauen**
   - Erg√§nze zus√§tzliche Filter/Redactions f√ºr sensible Inhalte (z.B. PII) und f√ºhre Logging/Alerting f√ºr geblockte Anfragen ein.
   - Erg√§nze UI-Hinweise, die bei Blockierungen eine verst√§ndliche Begr√ºndung liefern, und erweitere die Abdeckung um PII/Policy-bezogene Checks.

3. **Streaming & Kontext-Optimierung umsetzen**
   - Schalte Token-Streaming f√ºr Chat-Antworten ein und schreibe den Client auf inkrementelles Rendering um.
   - K√ºrze den System-Prompt/Verlauf (z.B. nur letzte N-Nachrichten) oder fasse √§ltere Eintr√§ge zusammen, um Tokenkosten zu senken.

4. **Beobachtbarkeit und Kostenkontrolle**
   - Sammle strukturierte Server-Logs (Latenzen, Fehlerraten, Token-Usage) und setze Warnungen/Alerts auf Ausrei√üer.
   - Erg√§nze einfache Concurrency-Limits pro Route, damit parallele Anfragen nicht unkontrolliert eskalieren.

5. **Regressionen fr√ºh erkennen**
   - F√ºhre `npm run lint` und `npm run build` lokal aus, bevor du neue Deployments anst√∂√üt.
   - Erg√§nze zeitnah automatisierte Tests (Unit + E2E) f√ºr Auth-Zwang, Ratenbegrenzung, Moderation und Streaming-Rendering.

## Weiterf√ºhrende Verbesserungen (Priorit√§t & Aufwand)

| Priorit√§t | Aufwand | Ma√ünahme | Nutzen |
| --- | --- | --- | --- |
| Hoch | Mittel | **Supabase RLS + Least-Privilege** ‚Äì Service-Role-Key aus Requests entfernen, Policies pro Tabelle/View definieren und Access-Tokens pro Nutzer ausstellen. | Minimiert Datenexfiltration und separiert Mandanten sauber. |
| Hoch | Mittel | **PII-Redaction & erweiterte Moderation** ‚Äì Eingaben/Transkripte vor dem Modell neutralisieren (z.‚ÄØB. E-Mail/Telefon) und blockierte Inhalte mit verst√§ndlichen UI-Hinweisen quittieren. | Reduziert Compliance-Risiken und liefert klare Nutzerf√ºhrung. |
| Mittel | Mittel | **Streaming + Kontextkompression** ‚Äì Token-Streaming aktivieren, √§ltere Verlaufsteile k√ºrzen oder zusammenfassen. | Schnellere wahrgenommene Antwortzeit und geringere Tokenkosten. |
| Mittel | Niedrig | **Beobachtbarkeit & Alerts** ‚Äì Strukturierte Logs (Latenz, Token, Fehlercodes), Dashboards und Alarme bei Ausrei√üern hinzuf√ºgen. | Schnellere Fehlersuche und Kostenkontrolle. |
| Mittel | Niedrig | **Concurrency-Governance** ‚Äì Pro-Route Parallelit√§tslimits und Backpressure einbauen, ggf. Supabase-Reads cachen. | Stabilere Responses unter Last und geringere Provider-Fehler. |
| Niedrig | Mittel | **UX & Barrierefreiheit** ‚Äì Tastatursteuerung, Fokus-Styles und klare Retry-Hinweise f√ºr STT/TTS erg√§nzen. | Bessere Nutzbarkeit f√ºr breitere Zielgruppen. |

## Browser-Unterst√ºtzung

- ‚úÖ Chrome (Desktop & Mobile)
- ‚úÖ Firefox (Desktop & Mobile)
- ‚úÖ Safari (iOS 14.3+, macOS Safari 11+)
- ‚ö†Ô∏è Safari auf macOS ben√∂tigt HTTPS f√ºr Mikrofon-Zugriff

## Lizenz

Private Projekt

